{
  "metadata": {
    "title": "CTAL-TAE 200 Q Bank",
    "counts": {
      "v20": 0,
      "astqb": 0,
      "v22": 39,
      "custom": 80,
      "total": 119
    }
  },
  "questions": [
    {
      "id": "v2.2-Q1",
      "question": "<p><strong>Which of the following is a limitation of test automation?</strong></p>",
      "answers": {
        "a": "Only usability tests can be automated effectively",
        "b": "Test automation can be executed only after system testing is done",
        "c": "Test automation can only check results that can be verified visually",
        "d": "Test automation can only check results that can be verified by code"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-1.1.1",
      "tip": "Automation is best at objective, code-verifiable checks; subjective aspects like usability require humans.",
      "real_life_example": "In a payments project, automation couldn’t judge whether a tooltip wording was confusing; it only verified functional outcomes and data correctness."
    },
    {
      "id": "v2.2-Q2",
      "question": "Which of the following is true about test automation and the SDLC?",
      "answers": {
        "a": "In Agile software development automated tests focus more on acceptance tests than on",
        "b": "In Agile software development automated tests focus more on component tests than on",
        "c": "In the V-model automated test execution must be performed after manual test execution",
        "d": "In the V-model implementation of test automation is performed throughout the whole"
      },
      "correct_answer": "b",
      "points": 1,
      "syllabus_reference": "TAE-1.2.1",
      "tip": "Think of the Test Pyramid: the base (unit/component tests) has the highest volume of automation, while acceptance/E2E tests are fewer. This helps ensure fast feedback and maintainable automation.",
      "real_life_example": "In Agile projects, successful automation suites typically have hundreds of unit/component tests running in CI/CD for rapid feedback, while only a limited number of end-to-end acceptance tests are automated. This balances speed, cost, and reliability."
    },
    {
      "id": "v2.2-Q3",
      "question": "Which one of the following factors is NOT necessary to consider when selecting suitable test tools?",
      "answers": {
        "a": "SUT architecture",
        "b": "Actual composition and experience of the test team",
        "c": "Licensing and support of the test tool",
        "d": "Quality of the SUT requirements"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-1.2.2",
      "tip": "Common, you do not need any tip to solve this.",
      "real_life_example": "Practical teams align automation to this LO to improve feedback and stability."
    },
    {
      "id": "v2.2-Q4",
      "question": "<p>When a system is designed for <strong>testability</strong>, one of the characteristics is that the test automation framework (TAF) can access interfaces to perform actions on the system.</p><p><strong>What is this characteristic called?</strong></p>",
      "answers": {
        "a": "Observability",
        "b": "Controllability",
        "c": "Maintainability",
        "d": "Interoperability"
      },
      "correct_answer": "b",
      "points": 1,
      "syllabus_reference": "TAE-2.1.1",
      "tip": "Controllability = ability to drive the SUT (e.g., APIs, hooks, stubs) to desired states or actions. Observability = ability to see what happened (logs/metrics). The question is about performing actions → Controllability.",
      "real_life_example": "For a healthcare registration service, engineers exposed admin APIs and feature flags so the TAS could create patients, toggle flows, and reset data. Those control interfaces made tests stable and fast—classic controllability. Observability (logs/metrics) was used separately to diagnose failures."
    },
    {
      "id": "v2.2-Q5",
      "question": "What type of test automation is mainly performed in the Preproduction environment?",
      "answers": {
        "a": "Component testing",
        "b": "Performance efficiency testing and user acceptance testing",
        "c": "Static analysis",
        "d": "Component Integration testing"
      },
      "correct_answer": "b",
      "points": 1,
      "syllabus_reference": "TAE-2.1.2",
      "tip": "Think about the purpose of Preproduction: it should mirror production and validate system readiness. That’s where performance efficiency tests and UAT fit best.",
      "real_life_example": "In e-commerce platforms, load and stress tests are executed in preproduction to ensure the system can handle Black Friday traffic. Business users also run UAT in the same environment to verify end-to-end processes before going live."
    },
    {
      "id": "Custom-Q6",
      "question": "<p><strong>Which of the following lists the major parts of a Test Automation Architecture (TAA)?</strong></p>",
      "answers": {
        "a": "Test Generation, Test Definition, Test Execution, Test Adaptation",
        "b": "Core Libraries Layer, Business Logic Layer, Test Scripts Layer, Test Data Layer",
        "c": "Tool setup, Environment setup, Logging framework, Reporting framework",
        "d": "Build pipeline integration, Version control, Continuous deployment, Defect management"
      },
      "correct_answer": "a",
      "points": 2,
      "syllabus_reference": "TAE-3.1.1",
      "tip": "The TAA defines four key capabilities: Test Generation, Test Definition, Test Execution, and Test Adaptation. Don’t confuse these with TAF layers or supporting infrastructure.",
      "real_life_example": "In a retail automation project, model-based tools generated test cases (Test Generation), engineers implemented them (Test Definition), execution logs were captured (Test Execution), and adapters connected tests to multiple APIs (Test Adaptation)."
    },
    {
      "id": "v2.2-Q7",
      "question": "<p>You are working for an IT company developing a <strong>built-in Android-based car multimedia system</strong>. The software contains several components working together, and developers are following a test-driven development approach.</p><p>After the development of the software, it is delivered to another IT company which integrates the software with the hardware elements and sells them to car manufacturers.</p><p><strong>Which of the following should be considered during capturing the test automation requirements by the IT company you are working for?</strong></p><p><em>Select TWO options.</em></p>",
      "answers": {
        "a": "Is it important for the test automation approach to support component testing?",
        "b": "Should the test automation approach support beta testing?",
        "c": "Is it important for the test automation approach to support the testing of the software in assembled car hardware?",
        "d": "Which tester roles should be supported by the test automation approach?",
        "e": "Is it important for the test automation approach to support the mobile application store release process?"
      },
      "correct_answer": ["a", "d"],
      "points": 3,
      "syllabus_reference": "TAE-2.2.1",
      "tip": "When capturing automation requirements, focus on what is within the developing company’s scope: supporting component-level automation and tester roles. Beta testing, hardware integration, or app store release are outside this scope.",
      "real_life_example": "In an automotive infotainment project, the development company ensured that component tests (e.g., audio playback, Bluetooth connection) were automated and that different tester roles (e.g., functional testers vs. integration testers) were supported. Integration with car hardware was left for the partner company."
    },
    {
      "id": "v2.2-Q8",
      "question": "<p>You are evaluating test automation tools. The following is a list of findings for one of the tools that were evaluated against the selection requirements:</p><ul><li>The tool has a very informative dashboard which shows all the relevant test information about the SUT.</li><li>The tool includes a test logging component which logs all the necessary information that follows test execution and helps troubleshoot problems found during the tests.</li><li>The tool includes a customizable test reporting component.</li><li>During the proof of concept, the tool performed <strong>very slowly</strong> against the SUT, compared to the other tools that were evaluated.</li><li>The current test environment is valid according to the release notes of the tool, which means it fulfills the hardware and software requirements.</li></ul><p><strong>What should be your next step regarding the selection of this tool?</strong></p>",
      "answers": {
        "a": "Acquire more hardware resources for the SUT to decrease the performance degradation",
        "b": "Turn off test logging to improve performance of the tool",
        "c": "Recommend not selecting this tool",
        "d": "Plan to migrate the SUT to another hardware/software environment where there is a possibility for better performance"
      },
      "correct_answer": "c",
      "points": 3,
      "syllabus_reference": "TAE-2.2.2",
      "tip": "A tool that fails its proof of concept because of performance issues should be rejected. Reporting and dashboards cannot compensate for poor execution speed, which will become a bottleneck in CI/CD pipelines.",
      "real_life_example": "In a telecom project, a tool with advanced reporting was considered. However, proof-of-concept runs showed it doubled execution time compared to alternatives. The team dropped it to avoid slowing down nightly regression and CI/CD feedback cycles."
    },
    {
      "id": "v2.2-Q9",
      "question": "<p><strong>Match the list of tasks in the Test Automation Architecture (TAA) capabilities below with their correct role names:</strong></p><ol><li>Mapping the abstract test cases to concrete test cases suitable for execution.</li><li>Implementation of test cases and/or test suites.</li><li>Test logging with detailed information about the test steps and actions.</li><li>Mechanism for connecting to the SUT via protocols and services.</li></ol><p>Roles:</p><ul><li>A. Test Definition</li><li>B. Test Adaptation</li><li>C. Test Generation</li><li>D. Test Execution</li></ul>",
      "answers": {
        "a": "1D, 2A, 3C, 4B",
        "b": "1C, 2A, 3B, 4D",
        "c": "1A, 2B, 3D, 4C",
        "d": "1C, 2A, 3D, 4B"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-3.1.1",
      "tip": "Match tasks to intent: <br>- Test Generation = mapping abstract to concrete cases <br>- Test Definition = implementing test cases <br>- Test Execution = logging and running <br>- Test Adaptation = connecting to the SUT.",
      "real_life_example": "In a banking platform, model-based tools generated executable tests from abstract workflows (Test Generation). Engineers coded them into suites (Test Definition). Runs produced detailed logs (Test Execution), while adapters connected tests to APIs and DB layers (Test Adaptation)."
    },
    {
      "id": "v2.2-Q10",
      "question": "<p>As a Test Automation Engineer, you have encountered a problem during the <strong>initial implementation</strong> of the test automation solution (TAS).</p><p>The output from the system tests could not be translated back to the automated test cases to determine the test results.</p><p><strong>Which option may have MOST LIKELY been a cause of the problem?</strong></p>",
      "answers": {
        "a": "The test tool libraries were NOT designed to be updated upon each SUT version release",
        "b": "The integration to the SUT system under test was NOT set up via APIs",
        "c": "The SUT-specific adaptors for the selected tool were NOT implemented",
        "d": "The SUT and the test management tool were NOT compatible"
      },
      "correct_answer": "c",
      "points": 1,
      "syllabus_reference": "TAE-3.1.2",
      "tip": "Adaptors act as translators between the SUT and the TAS. Without them, raw system outputs cannot be mapped back to automated test cases, making results meaningless.",
      "real_life_example": "In a retail POS project, test automation initially failed to interpret transaction logs. After building a custom adaptor that mapped SUT outputs into standardized test results, the TAS could evaluate pass/fail conditions reliably."
    },
    {
      "id": "v2.2-Q12",
      "question": "<p>You are working on a test automation project that is used to automate <strong>GUI testing</strong> of a web-based public transport service. The project has a <strong>limited timescale</strong>.</p><p>There are manual test cases which can be automated first. One of the goals is to <strong>implement test cases directly into the automated test scripts</strong>.</p><p><strong>Which technique or approach should be used for automating test cases to meet the goals?</strong></p>",
      "answers": {
        "a": "Using the keyword-driven test technique",
        "b": "Using the behavior-driven development approach",
        "c": "Using the capture/playback test automation approach",
        "d": "Using the data-driven test automation technique"
      },
      "correct_answer": "c",
      "points": 2,
      "syllabus_reference": "TAE-3.1.4",
      "tip": "Capture/playback is ideal when time is short and the goal is quick automation. Maintainability is weaker, but speed of implementation outweighs this trade-off.",
      "real_life_example": "In a metro ticketing project, the team used Selenium IDE’s record-and-playback to automate key ticket purchase flows within days. This gave stakeholders fast feedback, even though the scripts required later refactoring for maintainability."
    },
    {
      "id": "v2.2-Q13",
      "question": "<p>You join a company where <strong>manual testing is mature</strong>, but <strong>test automation has been abandoned</strong> for a while.</p><p>The testers have generated a <strong>massive amount of test data</strong> and are typically using <strong>10 to 20 variations per scenario</strong>.</p><p>After your initial review, you see that the <strong>Test Automation Framework (TAF)</strong> can easily be fixed, but the test cases need to be completely revamped.</p><p><strong>Which of the following test automation approaches should you choose to achieve great results quickly?</strong></p>",
      "answers": {
        "a": "Data-driven testing",
        "b": "Behavior-driven development",
        "c": "Capture/playback",
        "d": "Acceptance test-driven development"
      },
      "correct_answer": "a",
      "points": 2,
      "syllabus_reference": "TAE-3.1.4",
      "tip": "Large amounts of reusable test data with many variations are a perfect fit for <strong>data-driven testing</strong>. It maximizes efficiency by applying different datasets to the same test scripts, instead of duplicating logic.",
      "real_life_example": "In a banking project, thousands of account profiles had to be tested. Instead of writing separate scripts for each variation, engineers used a data-driven framework that injected datasets into a single script, achieving broad coverage in a fraction of the time."
    },
    {
      "id": "v2.2-Q14",
      "question": "<p>You are working on a test automation project that is used to automate GUI testing of an e-commerce site. The site contains a digital assistant which helps users to set up their accounts, their name, billing address, shipping address, and security credentials.</p><p>Currently, the development of the software is in a phase where usability testers check the digital assistant and give recommendations on necessary changes. This is done iteratively:</p><ul><li>The developers modify the graphical user interface (GUI)</li><li>The usability testers check the modifications</li><li>Usability testing is repeated</li></ul><p><strong>Which design pattern is the best implemented in this case?</strong></p>",
      "answers": {
        "a": "Implement the page object pattern and store all the user actions associated with the GUI",
        "b": "Implement the flow model pattern, store all the web elements in the relevant page models, and store all the user actions associated with the GUI elements in the relevant flow models",
        "c": "Implement the facade design pattern and provide interfaces for the GUI elements to hide the used internal locator mechanism",
        "d": "Implement the singleton design pattern and create a single piece of code to handle the locating of elements"
      },
      "correct_answer": "b",
      "points": 2,
      "syllabus_reference": "TAE-3.1.5",
      "tip": "Frequent UI changes make the flow model pattern ideal, since it separates element locators (page models) from user interactions (flows), reducing maintenance when the GUI is updated.",
      "real_life_example": "An online retailer’s checkout assistant changed its UI frequently during development. By applying the flow model pattern, testers only updated the flow logic without rewriting all test scripts, significantly cutting maintenance time."
    },
    {
      "id": "v2.2-Q15",
      "question": "<p>The senior management wants to implement a Test Automation Solution (TAS) in your organization and asks you to lead this initiative. You have been given directions to start a <strong>pilot project</strong>.</p><p><strong>Which of the following statements best describes the objective of this pilot project?</strong></p><ul><li>i. Document the SUT parts which have not been documented during the development</li><li>ii. Identify the metrics and the measurement methods to monitor the SUT in the production environment</li><li>iii. Analyze defects found during the testing of the TAS</li><li>iv. Evaluate licensing options and corporation rules</li><li>v. Select the most suitable commercial off-the-shelf or open-source tool</li></ul>",
      "answers": {
        "a": "i, ii and iii are valid objectives of the pilot project",
        "b": "ii and iv are valid objectives of the pilot project",
        "c": "i, ii and v are valid objectives of the pilot project",
        "d": "iv and v are valid objectives of the pilot project"
      },
      "correct_answer": "d",
      "points": 2,
      "syllabus_reference": "TAE-4.1.1",
      "tip": "The purpose of a pilot project is to validate tool feasibility and organizational fit. Focus on evaluating licensing, rules, and selecting a suitable tool, not on documenting SUT or analyzing defects.",
      "real_life_example": "A fintech company ran a pilot project with a selected open-source framework and a commercial tool. The pilot compared licensing implications, CI/CD integration, and ease of use. Based on results, the team confidently selected the commercial tool for wider rollout."
    },
    {
      "id": "v2.2-Q16",
      "question": "<p>You are tasked with implementing a Test Automation Solution (TAS) for functional suitability tests that must be executed automatically after each daily build. The TAS needs to integrate seamlessly with the existing CI/CD pipeline and provide quick feedback on the software's quality.</p><p>However, you've identified several potential risks that could impact the successful deployment and operation of the TAS. As the test automation engineer, you need to analyze the deployment risks associated with implementing the TAS and determine the <strong>BEST mitigation strategies</strong> for each risk.</p><p><strong>Which of the following BEST matches the deployment risks with their appropriate mitigation strategies?</strong></p><ul><li>1. Test execution not triggered by the build</li><li>2. Only the full test suite can be executed</li><li>3. Test data unavailable when starting the test</li><li>4. Difficulty in troubleshooting failed tests</li></ul><p>Mitigation strategies:</p><ul><li>A. Log detailed information during test execution</li><li>B. Integrate test automation into the CI/CD pipeline</li><li>C. Use third-party tools to generate test data</li><li>D. Utilize test harnesses and test fixtures</li></ul>",
      "answers": {
        "a": "1-B, 2-A, 3-D, 4-C",
        "b": "1-A, 2-B, 3-C, 4-D",
        "c": "1-B, 2-D, 3-C, 4-A",
        "d": "1-D, 2-B, 3-C, 4-A"
      },
      "correct_answer": "c",
      "points": 3,
      "syllabus_reference": "TAE-4.2.1",
      "tip": "Match risks to their logical solutions: CI/CD integration fixes missing triggers, harnesses/fixtures enable partial runs, data tools solve availability issues, and detailed logs help troubleshoot failures.",
      "real_life_example": "In a SaaS project, regression runs initially blocked the pipeline because only full suites could be executed. By introducing test harnesses, the team ran smaller, targeted subsets on each build, while full suites ran nightly, reducing feedback time dramatically."
    },
    {
      "id": "v2.2-Q17",
      "question": "<p>Which one of the following is an important factor to improve code maintainability?</p>",
      "answers": {
        "a": "Define generic functions with all the necessary parameters",
        "b": "Let developers uniquely name code variables",
        "c": "Use static analyzers to keep the code clean",
        "d": "Hardcode values to easily understand their meaning"
      },
      "correct_answer": "c",
      "points": 1,
      "syllabus_reference": "TAE-4.3.1",
      "tip": "Static analyzers enforce coding standards, detect code smells, and prevent poor practices early, which directly improves long-term maintainability.",
      "real_life_example": "In a payments automation project, running SonarQube flagged duplicated code and unused variables in test scripts. Fixing these issues reduced maintenance overhead and made onboarding new engineers much easier."
    },
    {
      "id": "v2.2-Q18",
      "question": "<p>What is the most effective way to reduce the maintenance time for test automation code?</p>",
      "answers": {
        "a": "Keep the code outside of a version control system",
        "b": "Embed static data directly within the test scripts for easier access",
        "c": "Apply design patterns in your test automation framework",
        "d": "Regularly rewrite test scripts from scratch instead of reusing existing components"
      },
      "correct_answer": "c",
      "points": 1,
      "syllabus_reference": "TAE-4.3.1",
      "tip": "Design patterns like Page Object or Flow Model reduce duplication and isolate changes, which cuts down maintenance effort.",
      "real_life_example": "An e-commerce team adopted the Page Object pattern in Selenium. When product page locators changed, they only updated the page model file instead of fixing dozens of individual test scripts, saving significant maintenance time."
    },
    {
      "id": "v2.2-Q19",
      "question": "<p>There is an IT company which develops an often-changing financial software product using the Agile software development model. The development, integration, and deployment processes are highly automated. There is already a CI/CD pipeline established.</p><p>You are working on a Test Automation Solution (TAS). The goal is to create a TAS which can be used for as many test automation purposes as possible.</p><p><strong>Which of the following options are valid purposes for test automation in this case?</strong></p>",
      "answers": {
        "a": "Run a regression test suite every night",
        "b": "Execute a build of a component",
        "c": "Run a static code analysis",
        "d": "Execute an automated performance efficiency test in the CI/CD pipeline",
        "e": "Package and deploy the application as part of the deployment phase"
      },
      "correct_answer": ["a", "d"],
      "points": 2,
      "syllabus_reference": "TAE-5.1.1",
      "tip": "Focus on testing activities that provide feedback on software quality (e.g., regression and performance). Building, packaging, or static analysis are important, but they are not test automation purposes.",
      "real_life_example": "In a fintech CI/CD pipeline, nightly regression runs validated frequent code changes, while automated performance tests ensured trading transactions stayed within strict latency limits before every release."
    },
    {
      "id": "v2.2-Q20",
      "question": "<p><strong>Which statement is correct?</strong></p>",
      "answers": {
        "a": "Tests are not executed as part of the deployment phase",
        "b": "Tests are not executed as a separate pipeline, triggered by the successful deployment",
        "c": "Test cases do not act as a quality gate when different automated test suites will run on each deployment",
        "d": "Pipelines are not recommended for regression testing due to the extensive scope and size of these tests"
      },
      "correct_answer": "c",
      "points": 2,
      "syllabus_reference": "TAE-5.1.1",
      "tip": "Look for the subtle truth: automated test cases may run during deployments, but they don’t always act as hard quality gates when multiple test suites run across environments.",
      "real_life_example": "In a CI/CD pipeline for an insurance app, deployments triggered smoke, regression, and performance suites. Although tests ran, no single suite acted as a strict quality gate — instead, different suites ran at different pipeline stages."
    },
    {
      "id": "v2.2-Q21",
      "question": "<p><strong>How is configuration management used in test automation?</strong></p>",
      "answers": {
        "a": "It enables the management of test data and test environment configurations",
        "b": "The SUT configuration can be stored and easily removed",
        "c": "It enables management of user rights for accessing test automation",
        "d": "Test automation results can easily be analyzed"
      },
      "correct_answer": "a",
      "points": 1,
      "syllabus_reference": "TAE-5.1.2",
      "tip": "Configuration management in test automation ensures consistency of test data, environment settings, and deployment parameters — not user rights or results analysis.",
      "real_life_example": "In a telecom project, test environment URLs and credentials were versioned in configuration management. This ensured that automated regression suites always ran against the correct environment without manual setup errors."
    },
    {
      "id": "v2.2-Q22",
      "question": "<p><strong>Which item below is NOT part of the test environment configuration?</strong></p>",
      "answers": {
        "a": "Uniform resource locators (URLs)",
        "b": "Credentials",
        "c": "Test data",
        "d": "Common core library"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-5.1.2",
      "tip": "Test environment configuration deals with runtime variables (URLs, credentials, data). Framework code like a core library is part of the automation solution, not the environment.",
      "real_life_example": "In a banking project, URLs, login credentials, and anonymized customer datasets were managed per environment (test, staging, preprod). The core automation libraries stayed the same across environments and were not treated as configuration."
    },
    {
      "id": "v2.2-Q23",
      "question": "<p><strong>How does contract testing NOT contribute to API test automation dependencies in an infrastructure?</strong></p>",
      "answers": {
        "a": "Ensures that APIs adhere to predefined contracts for communication",
        "b": "Can be used to test the communication of microservices",
        "c": "Validates the compatibility of two separate systems",
        "d": "Verifies whether a system satisfies its contractual requirements"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-5.1.3",
      "tip": "Contract testing focuses on API communication agreements (schemas, endpoints, compatibility). It does not validate whether the whole system meets business contractual obligations.",
      "real_life_example": "At a bank, contract tests ensured API schema changes between account and payment services didn’t break communication. However, whether the banking system met regulatory requirements was validated through separate compliance and acceptance testing, not contract testing."
    },
    {
      "id": "v2.2-Q24",
      "question": "<p>You are on a project where the teams are working on breaking down an old monolithic web service into several microservices.</p><p><strong>Which of the following documents can assist you in identifying dependencies and developing your Test Automation Solution (TAS) for API testing?</strong></p><ul><li>i. Application programming interface (API) specification</li><li>ii. System architecture diagram</li><li>iii. Test strategy</li><li>iv. Release notes</li></ul>",
      "answers": {
        "a": "i, ii, and iv",
        "b": "i and ii",
        "c": "ii, iii, and iv",
        "d": "i"
      },
      "correct_answer": "b",
      "points": 1,
      "syllabus_reference": "TAE-5.1.3",
      "tip": "For API dependency analysis, focus on technical design artifacts: API specifications define endpoints and data contracts, while architecture diagrams reveal microservice interactions. Strategy or release notes won’t help identify dependencies. Test strategy defines approach, not dependencies; release notes describe changes, not architecture",
      "real_life_example": "When a retail platform split its checkout system into microservices, testers used the API specs to define contract tests and the architecture diagram to map service dependencies (e.g., payments → inventory → shipping). This ensured the TAS covered all critical service interactions."
    },
    {
      "id": "v2.2-Q25",
      "question": "<p>You are working on a test automation project that is used to automate GUI testing of an online web shop. The web shop contains a wizard which helps users to set up their accounts, their name, billing address, shipping address, and security credentials.</p><p>During the test automation, the steps of the wizard are recorded first. Screenshots are taken and stored during these steps, which we consider the baseline. The baseline was then rerun with no change to the SUT and all tests passed.</p><p>After the developers submit a change to the wizard, the recorded test scripts are played back, and the screenshots of each step are compared with the baseline screenshots. During playback, <strong>all test cases failed even though the visible content seems unchanged</strong>.</p><p><strong>What could be the cause?</strong></p>",
      "answers": {
        "a": "An internally used technical session ID is also recorded, which changes during the playback. This should be substituted with variables to avoid failures.",
        "b": "Screenshots are not correctly associated with the test steps, leading to comparisons being made in the wrong order.",
        "c": "The failure occurred because relying solely on screenshot comparisons led to inaccurate results.",
        "d": "The date in the GUI header is different from the date when it was recorded. This date field should be excluded from the comparison.",
        "e": "Screenshots were captured in low resolution, causing details to be blurred, making the comparison unreliable."
      },
      "correct_answer": ["a", "d"],
      "points": 2,
      "syllabus_reference": "TAE-6.1.1",
      "tip": "Think about dynamic values: session IDs and dates change between runs. If these aren’t excluded or parameterized, screenshot comparisons will fail even if the UI hasn’t changed.",
      "real_life_example": "In an airline booking portal, screenshot-based tests failed daily because session tokens and date headers changed every run. After parameterizing session IDs and ignoring date fields, the GUI tests stabilized."
    },
    {
      "id": "v2.2-Q26",
      "question": "<p>You are working on a project where you are responsible for extending the current Test Automation Framework (TAF) that is used for web service testing, with additional test logging capabilities.</p><p>The TAF uses a third-party tool to create file logs and an HTML report to quickly visualize the test results. In the test implementation, various dynamic values are used to generate the actual test data, and the SUT (a web service without a UI) connects to multiple legacy test systems. Unfortunately, the tests are very unstable, and you need to add meaningful information to the test logging to better analyze the reasons for the failures.</p><p><strong>What additional information should <em>not</em> be included in the test logging to make it more useful, and why?</strong></p>",
      "answers": {
        "a": "Timestamps should be included in the test logs to see if the failure connects to a given legacy system outage",
        "b": "Screenshots should be included to see actual request-response pairs",
        "c": "Randomly generated values should be logged to allow for analysis of the actual results during test executions",
        "d": "In case of assertion failures, meaningful information like actual results versus expected results should be logged instead of stack traces"
      },
      "correct_answer": "b",
      "points": 2,
      "syllabus_reference": "TAE-6.1.1",
      "tip": "For web service testing without a UI, screenshots don’t add value. Instead, focus on timestamps, logged request/response data, and assertion details for effective debugging.",
      "real_life_example": "In a banking API test suite, screenshots were initially captured for failed calls, but they only wasted storage and added noise. After switching to detailed request/response logging with timestamps and IDs, the team quickly identified flaky interactions with legacy systems."
    },
    {
      "id": "v2.2-Q27",
      "question": "<p>During the design phase of a performance-critical software product, management decides to create a Test Automation Solution (TAS) to do load testing on the software to measure its performance.</p><p>The product contains different architectural components, including:</p><ul><li>A browser-based front end</li><li>Microservices implemented in the back end</li><li>A relational database</li></ul><p>It is important to measure all individual transactions in the entire architectural stack.</p><p><strong>How can you automatically provide this information from the test automation software?</strong></p>",
      "answers": {
        "a": "You cannot. This information has to be gathered manually at the end of every transaction",
        "b": "The test automation engineer (TAE) can record the timing information during the execution",
        "c": "Trace IDs should be populated across the software components and measured time values should be collected for each transaction",
        "d": "Third-party tools should be inserted into the different layers of the architecture and these tools should manually aggregate results"
      },
      "correct_answer": "c",
      "points": 3,
      "syllabus_reference": "TAE-6.1.2",
      "tip": "To trace performance across the full stack, you need unique transaction identifiers (Trace IDs). They let you follow a single request across front end, services, and database while collecting timings automatically.",
      "real_life_example": "In a payment processing platform, trace IDs were added to each transaction. This allowed testers to correlate response times from the UI, APIs, and database, making it clear where latency spikes originated."
    },
    {
      "id": "v2.2-Q28",
      "question": "<p>As a Test Automation Engineer, you are tasked with demonstrating to management whether the <strong>test success rate is improving</strong> over a series of test runs.</p><p><strong>What is the right approach to fulfill this requirement?</strong></p>",
      "answers": {
        "a": "Compare the test results with the expected results",
        "b": "Use traffic light indicators to show test execution progress",
        "c": "Detailed test reports with percentages of test completion",
        "d": "Implement an analysis feature that compares previous test results and highlights trends"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-6.1.3",
      "tip": "Management needs visibility of trends, not just single-run results. Historical comparison and trend analysis clearly show whether quality is improving.",
      "real_life_example": "In a SaaS project, Jenkins pipelines published trend graphs comparing pass/fail rates across builds. Management could easily see defect fixes improving success rates week by week."
    },
    {
      "id": "v2.2-Q29",
      "question": "<p>Your team has developed a Test Automation Solution (TAS) for a computer-aided design (CAD) software program. This software has several different versions in production and has been ported to different languages and platforms.</p><p>Manual testers have been using the TAS on their local computers with different language settings, versions, and platforms. Before proceeding with further automated testing, <strong>you need to verify the test automation environment</strong>.</p><p><strong>What is an important consideration when verifying the environment for this TAS?</strong></p>",
      "answers": {
        "a": "Establish a central repository to verify that all testers are using the same version of the tool and can access it correctly",
        "b": "Create and document a procedure to verify the proper installation of the CAD software on each tester's machine",
        "c": "Set up a central test environment where the CAD software is installed and verify that automation scripts can interact with it from each local computer",
        "d": "Use configuration management to verify the consistency of test data and scripts across different environments"
      },
      "correct_answer": "a",
      "points": 2,
      "syllabus_reference": "TAE-7.1.1",
      "tip": "In distributed setups, version mismatches of tools cause unstable results. A central repository ensures all testers use the same tool version, avoiding 'works on my machine' issues.",
      "real_life_example": "In a CAD automation project, testers running different tool versions produced inconsistent results. By enforcing tool access via a central repository, the team standardized execution and eliminated environment-related failures."
    },
    {
      "id": "v2.2-Q30",
      "question": "<p>You work in a software development team that requires testing to occur in many different test environments. Your manager has expressed that the team is spending a significant amount of time overcoming <strong>false-positive results</strong> because the Test Automation Solution (TAS) is not configured correctly when using it in a new test environment.</p><p>Additionally, there appear to be <strong>version differences</strong> when comparing the TAS in each test environment. Even new test environments sometimes are set up with very old TAS components.</p><p><strong>Which TWO options would help address this situation?</strong></p>",
      "answers": {
        "a": "Create an automated installation script for the tools and configurations that make up the TAS",
        "b": "Limit the TAS to only be used in the most important test environments",
        "c": "Utilize a repository to store the TAS that is accessible to all test environments",
        "d": "Leverage manual testing to verify that the TAS has been configured properly in all test environments",
        "e": "Due to time constraints skip the implementation of component tests for the TAS"
      },
      "correct_answer": ["a", "c"],
      "points": 2,
      "syllabus_reference": "TAE-7.1.1",
      "tip": "Automating TAS installation and centralizing its components ensures consistency across environments. Manual verification or limiting environments does not solve root causes.",
      "real_life_example": "In a healthcare SaaS project, teams faced flaky results because different test labs had mismatched TAS versions. By introducing a central TAS repository and automated install scripts, every new environment was configured consistently, eliminating 'works on my machine' issues."
    },
    {
      "id": "v2.2-Q31",
      "question": "<p>You are about to verify an automated test suite. During the verification process you have found that <strong>some test scripts pass at one time and fail at another</strong>, therefore not providing reliable test results.</p><p><strong>What should you do to verify the validity of your test scripts?</strong></p>",
      "answers": {
        "a": "This is due to the parallel execution of the test scripts; synchronization would solve the issue",
        "b": "Re-execute the automated test suite and analyze the test results again",
        "c": "Remove the test scripts from the automated test suite and analyze them separately",
        "d": "This happens because several test scripts are using the same test data, so the separation of data would solve the issue"
      },
      "correct_answer": "c",
      "points": 1,
      "syllabus_reference": "TAE-7.1.2",
      "tip": "When scripts behave inconsistently, isolate them from the main suite. Running them separately helps identify if instability is caused by data, environment, or timing issues.",
      "real_life_example": "In a retail automation project, flaky checkout tests polluted regression results. By pulling them out of the suite and running them independently, the team discovered hidden environment dependencies and fixed them before reintegration."
    },
    {
      "id": "v2.2-Q32",
      "question": "<p>You have a test suite containing <strong>25 automated tests</strong> that verify the login functionality of an application’s home page. The test suite is executed at the end of each two-week sprint cycle for regression test purposes.</p><p>You notice that <strong>two test cases</strong> out of the 25 may sometimes cause a race condition in the application or receive a random error.</p><p><strong>What action should you take for these two test cases?</strong></p>",
      "answers": {
        "a": "Take no action because sometimes they execute successfully",
        "b": "Reduce the amount of test cases in the test suite from 25 to 15 and see if the test suite stabilizes",
        "c": "Remove the two test cases from the active test suite and analyze them separately to find the cause of instability",
        "d": "Replace the two test cases with ones that pass repeatedly so that the test suite still has 25 cases"
      },
      "correct_answer": "c",
      "points": 1,
      "syllabus_reference": "TAE-7.1.2",
      "tip": "Flaky tests reduce trust in the suite. Isolate and analyze unstable cases separately to identify root causes without affecting regression reliability.",
      "real_life_example": "In an e-commerce project, two login tests occasionally failed due to session handling race conditions. By isolating and debugging them outside the suite, engineers discovered a token refresh timing issue and fixed it without polluting regression results."
    },
    {
      "id": "v2.2-Q33",
      "question": "<p>You are working on a project to automate a regression test suite. When the regression test suite was executed manually last time, all the tests passed. But when you execute them via the Test Automation Solution (TAS), you find there are some failed tests.</p><p><strong>What should you do to handle this situation?</strong></p>",
      "answers": {
        "a": "Analyze log files to identify the root cause of the problem",
        "b": "Eliminate these test cases from the automated test suite, so the remaining tests can pass",
        "c": "Open a defect for the SUT as the failed tests are indicating an SUT problem",
        "d": "This is normal because automated tests behave differently than manual tests"
      },
      "correct_answer": "a",
      "points": 1,
      "syllabus_reference": "TAE-7.1.3",
      "tip": "Don’t assume the failures are SUT issues or dismiss them as 'normal.' First step is always root cause analysis using detailed logs to distinguish between automation problems and genuine defects.",
      "real_life_example": "In a logistics system, automated regression tests flagged failures in shipment creation. Logs revealed the issue was due to outdated test data, not an SUT defect. After fixing the test setup, results aligned with manual execution."
    },
    {
      "id": "v2.2-Q34",
      "question": "<p>You are preparing to execute a test automation suite for a <strong>security-critical application</strong> which must fulfill the highest security requirements.</p><p><strong>Which approach should you follow to verify the test automation code?</strong></p>",
      "answers": {
        "a": "Search the test logs for possible credential data",
        "b": "Eliminate test cases using sensitive test data",
        "c": "Execute the test suite slowly and methodically to check if there are any security vulnerabilities",
        "d": "Use a static analysis tool to identify security vulnerabilities"
      },
      "correct_answer": "d",
      "points": 1,
      "syllabus_reference": "TAE-7.1.4",
      "tip": "Static analysis tools are the most effective way to detect hardcoded secrets, insecure code patterns, or data leaks in test automation scripts before execution.",
      "real_life_example": "In a banking project, SonarQube flagged hardcoded credentials in a test automation repository. Fixing these issues before execution prevented a potential security breach and ensured compliance with internal audits."
    },
    {
      "id": "v2.2-Q35",
      "question": "<p>You are working on a test automation project that is used to automate graphical user interface (GUI) testing of an online web shop. The web shop contains a wizard which helps users set up their accounts: name, billing address, shipping address, and security credentials.</p><p>Currently, the development of the software is in a phase where usability testers check the wizard and give recommendations about the necessary changes. This is done iteratively:</p><ul><li>Developers modify the GUI</li><li>Usability testers check the modifications</li><li>Usability testing is repeated</li></ul><p>The test automation is mainly focusing on <strong>maintenance testing</strong>. In these UI-based test cases, data also includes UI locator values. An existing problem is that developers often change the internal identifiers of UI elements, so maintaining tests requires a lot of effort.</p><p><strong>Which of the following could be an important opportunity for improvement?</strong></p>",
      "answers": {
        "a": "Apply schema validation, which checks if mandatory response elements are present on the server side",
        "b": "Improve test logging to include information about UI elements and their locators to easily trace failures",
        "c": "Create a test histogram, which enables the TAEs to identify and select test cases that are most valuable to execute",
        "d": "Use an artificial intelligence (AI) algorithm based on machine learning and image recognition to identify UI elements even when locators change"
      },
      "correct_answer": "d",
      "points": 2,
      "syllabus_reference": "TAE-8.1.1",
      "tip": "Locator changes are a common pain point in UI automation. AI/ML-based image recognition can reduce maintenance effort by identifying elements visually instead of relying only on fragile locators.",
      "real_life_example": "A retail company integrated an AI-powered test automation tool (Applitools + Testim). Even when developers refactored the UI and changed internal IDs, the AI recognized elements visually, cutting locator maintenance time by more than 60%."
    },
    {
      "id": "v2.2-Q36",
      "question": "<p>Your organization maintains a regression test suite of over <strong>1000 automated test cases</strong> that has been extremely reliable over the years. Recently, the development team has decided to modernize their technology stack and are currently refactoring how their front end operates.</p><p>You notice that the application is now far more <strong>API-driven</strong> than the previous version, and this has an impact on how UI elements render. You anticipate this will affect the success rate of about <strong>75% of your automated test cases</strong>.</p><p><strong>What data analysis approaches should you use to determine how to fix your impacted automated test cases?</strong></p>",
      "answers": {
        "a": "Run the test cases several times in a CI/CD pipeline, perform visual report analysis, and identify recurring failures manually",
        "b": "Use AI algorithms and API schema validation tools",
        "c": "Recreate automated test cases to replace the ones that are not working properly and will continue to fail",
        "d": "Avoid automating certain test cases after analyzing exception logs, screenshots, and error messages"
      },
      "correct_answer": "b",
      "points": 2,
      "syllabus_reference": "TAE-8.1.1",
      "tip": "When modernizing a system, AI-powered analysis helps detect patterns in failing tests, while API schema validation ensures contracts are correct. This combination enables smarter adaptation instead of brute-force rewriting.",
      "real_life_example": "In a travel booking platform, 70% of UI tests broke after a new API-driven front end was introduced. By combining AI-driven root cause analysis with schema validation, the QA team quickly mapped which failures were due to locator issues and which were due to API contract changes, saving weeks of rework."
    },
    {
      "id": "v2.2-Q37",
      "question": "<p>You are working on an automated regression test suite that <strong>takes too long to execute</strong>, and its execution does not finish overnight. The test environment is only available for regression testing during the night.</p><p>Running multiple suites in parallel is not an option, as the target system is expensive and exists only as a single instance.</p><p><strong>What should be your next steps to ensure the test suite execution finishes overnight?</strong></p>",
      "answers": {
        "a": "Split the test suite into multiple parts, executing the parts on different nights of the week",
        "b": "Isolate test result verification from the test execution and start the verification process after execution completes",
        "c": "Rewrite the tests using the keyword-driven technique as that will be executed faster",
        "d": "Remove some tests from the test suite to reduce overall execution time",
        "e": "Remove any duplicate tests from the test suite"
      },
      "correct_answer": ["a", "e"],
      "points": 3,
      "syllabus_reference": "TAE-8.1.2",
      "tip": "When execution time is too long, prioritize optimization: remove duplicates and spread execution logically across available windows. Avoid risky shortcuts like dropping valuable tests or blindly rewriting them.",
      "real_life_example": "In a telecom billing system, regression runs exceeded the overnight test window. The team identified ~15% duplicate tests and removed them, then split the suite into two alternating nightly runs, keeping test coverage intact while meeting execution constraints."
    },
    {
      "id": "v2.2-Q38",
      "question": "<p>As a Test Automation Engineer (TAE), you are evaluating <strong>new versions of core libraries</strong>.</p><p><strong>Which is the correct order that can help you achieve these results?</strong></p>",
      "answers": {
        "a": "Create adoption plan → Determine impact → Update dependencies → Perform pilot",
        "b": "Perform pilot → Determine impact → Create adoption plan → Update dependencies",
        "c": "Update dependencies → Determine impact → Perform pilot → Create adoption plan",
        "d": "Determine impact → Update dependencies → Create adoption plan → Perform pilot"
      },
      "correct_answer": "b",
      "points": 3,
      "syllabus_reference": "TAE-8.1.2",
      "tip": "Always test first: start with a pilot on a controlled scope, then measure the impact, plan adoption, and finally roll out updates. Updating blindly or planning without evidence risks instability.",
      "real_life_example": "In a retail automation framework upgrade, a small pilot run with Selenium 4 exposed locator deprecations. After measuring impact, the team created an adoption plan and safely updated all dependencies across projects."
    },
    {
      "id": "v2.2-Q39",
      "question": "<p>You have been performing a quality review of a Test Automation Solution (TAS) to optimize the interaction of controls within the GUI.</p><p>The GUI includes several types of controls (e.g., dropdown list, checkbox, text field). There are also separate functions in the test scripts which act upon the different types of GUI controls to gather information and set values (e.g., visible/not visible, enabled/not enabled).</p><p><strong>Which of the following steps should you consider to increase the efficiency of the TAS?</strong></p>",
      "answers": {
        "a": "Separate the testing of the controls based on their types into different test suites",
        "b": "Research if there is a test automation tool which can replace the current solution",
        "c": "Check if there are any functions that can work with several types of controls, and consolidate them to reduce redundancy",
        "d": "Use the new operating system functions in the test scripts to handle the GUI controls"
      },
      "correct_answer": "c",
      "points": 2,
      "syllabus_reference": "TAE-8.1.3",
      "tip": "Efficiency improves when you reduce redundancy. Consolidating generic functions for multiple control types minimizes code duplication and maintenance overhead.",
      "real_life_example": "In an insurance web portal, the TAS initially had separate functions for text fields, dropdowns, and checkboxes. By consolidating them into a generic 'interactWithControl' function, the framework reduced duplicated code and cut maintenance time by 40%."
    },
    {
      "id": "v2.2-Q40",
      "question": "<p>As a Test Automation Engineer, you have automated the performance test of a customer management system. To effectively test the performance of this system, you need to create <strong>diverse test data</strong> that includes customers with varying profiles based on different input criteria.</p><p><strong>What is the best way to implement such a solution?</strong></p>",
      "answers": {
        "a": "Employ a test automation tool to invoke a web service API that creates new user profiles with the required variations",
        "b": "Register these users manually via the GUI, so the GUI functionality can also be tested",
        "c": "Use the production database during the test as it has the real volume and type of data",
        "d": "Implement a test automation script to anonymize customer data before using it during the performance test"
      },
      "correct_answer": "a",
      "points": 1,
      "syllabus_reference": "TAE-8.1.4",
      "tip": "Performance tests need large, varied, and reproducible datasets. Automating test data creation via APIs is the fastest, most scalable, and most reliable approach.",
      "real_life_example": "In a telecom project, engineers used automated scripts to call customer profile APIs and generate thousands of realistic accounts with different tariffs and usage patterns. This ensured the performance test reflected real-world load without risking production data."
    },
    {
      "id": "Custom-Q1",
      "question": "<p><strong>In the context of robustness, which option is MOST appropriate?</strong></p>",
      "answers": {
        "a": "Follow good practice for scalability, ensuring the solution can adapt to growth and change",
        "b": "Ignore maintainability, focusing only on immediate needs",
        "c": "Defer tests until later phases to save time early on",
        "d": "Rely only on manual checks instead of automated coverage"
      },
      "correct_answer": "a",
      "points": 3,
      "syllabus_reference": "TAE-3.1.5",
      "tip": "Robustness means the TAS can withstand changes and scale over time. That’s achieved by following scalability and maintainability best practices, not by cutting corners.",
      "real_life_example": "In an online banking project, the automation framework was designed with scalable patterns (Page Objects + service stubs). When the app expanded to new countries, the TAS scaled with minimal refactoring, demonstrating robustness."
    },
    {
      "id": "Custom-Q2",
      "question": "<p>You are designing a Test Automation Solution (TAS) that must be <strong>robust against frequent environment instability</strong>, such as temporary network outages and slow service responses.</p><p><strong>Which approach BEST supports robustness in this context?</strong></p>",
      "answers": {
        "a": "Implement retry mechanisms with backoff strategies for unstable operations",
        "b": "Disable automated tests until the environment stabilizes",
        "c": "Rely only on manual reruns of failed test cases",
        "d": "Ignore intermittent failures to save execution time"
      },
      "correct_answer": "a",
      "points": 3,
      "syllabus_reference": "TAE-3.1.5",
      "tip": "Robust TAS design handles instability gracefully. Retries with backoff avoid false negatives caused by transient issues without masking genuine defects.",
      "real_life_example": "In a retail mobile app, API calls for inventory sometimes timed out under load. Adding retry-with-exponential-backoff to the TAS reduced flaky failures by 70%, improving reliability without slowing the pipeline."
    },
    {
      "id": "Custom-Q3",
      "question": "<p>You are working on a test automation project for a <strong>hospital patient tracking service</strong>. The application handles a variety of data, including patient disease types, and supports actions such as:</p><ul><li>Register patient</li><li>Evaluate patient</li><li>Delete registration to move the patient to the next care step</li></ul><p>The application has been live for a long time and only minor changes are expected in some registration fields.</p><p><strong>Which test automation approach should you use?</strong></p>",
      "answers": {
        "a": "Linear scripting",
        "b": "Test-driven development",
        "c": "Keyword-driven development",
        "d": "Behavior-driven development"
      },
      "correct_answer": "c",
      "points": 2,
      "syllabus_reference": "TAE-3.1.4",
      "tip": "Keyword-driven frameworks are ideal for mature systems with stable flows but evolving fields. Keywords isolate business actions from technical implementation, reducing maintenance effort.",
      "real_life_example": "In a hospital project, test flows like 'Register Patient' and 'Evaluate Patient' were implemented as reusable keywords. When a new optional insurance field was added, only the 'Register Patient' keyword needed updating, not dozens of test scripts."
    },
    {
      "id": "Custom-Q4",
      "question": "<p>You are running a <strong>long regression test automation suite</strong>. You already added logs for:</p><ul><li>Start/end of the suite</li><li>Start/end of each test</li></ul><p>During execution you noticed that the <strong>last tests are taking longer</strong> than the first ones. To evaluate this, you re-ordered the suite and ran the last 10 tests first — their performance improved by 50%.</p><p><strong>Which additional log would help you identify the root cause?</strong></p>",
      "answers": {
        "a": "Screenshots of test execution",
        "b": "More detailed functional logs",
        "c": "System performance logs",
        "d": "TAS efficiency logs"
      },
      "correct_answer": "c",
      "points": 2,
      "syllabus_reference": "TAE-6.1.2",
      "tip": "When test performance degrades over time, look at system resource usage (CPU, memory, DB connections). System performance logs reveal bottlenecks better than functional or TAS-only logs.",
      "real_life_example": "In a hospital scheduling system, regression tests slowed down after a few hours. System performance logs showed a database connection pool leak. Fixing the resource issue restored stable test execution times."
    },
    {
      "id": "Custom-Q5",
      "question": "<p><strong>Which of the following are the major parts of a Test Automation Architecture (TAA)?</strong></p>",
      "answers": {
        "a": "Test Generation, Test Definition, Test Execution, Test Adaptation",
        "b": "Core libraries, Business logic layer, Test scripts layer, Test data layer",
        "c": "Tool setup, Environment setup, Logging framework, Reporting framework",
        "d": "Build pipeline integration, Version control, Continuous deployment, Defect management"
      },
      "correct_answer": "a",
      "points": 2,
      "syllabus_reference": "TAE-3.1.1",
      "tip": "Don’t confuse TAA capabilities (Generation, Definition, Execution, Adaptation) with TAF layers or supporting infrastructure like CI/CD or version control.",
      "real_life_example": "In a retail automation project, model-based tools generated test cases (Test Generation), engineers implemented them (Test Definition), execution logs were captured (Test Execution), and adapters connected tests to multiple APIs (Test Adaptation)."
    }
  ]
}
